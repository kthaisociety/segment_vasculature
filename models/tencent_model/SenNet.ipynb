{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/segment_vasculature/models\n"
     ]
    }
   ],
   "source": [
    "# Hack to import helper packages\n",
    "%cd /workspaces/segment_vasculature/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.loss_functions import DiceLoss\n",
    "from helpers.train_test import train_and_test\n",
    "from helpers.dataset_setup import train_test_split, augment_image, DataLoader, TRAIN_FOLDER, preprocess_image, preprocess_mask\n",
    "\n",
    "from resnet import resnet50\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = f'{TRAIN_FOLDER}/kidney_2/images/'\n",
    "image_files = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(\".tif\")]\n",
    "image_files.sort()\n",
    "labels_folder = f'{TRAIN_FOLDER}/kidney_2/labels/'\n",
    "label_files = [os.path.join(labels_folder, img) for img in os.listdir(labels_folder) if img.endswith(\".tif\")]\n",
    "label_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    # img = np.tile(img[...,None],[1,1,3])\n",
    "    img = img.astype('float32')\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img/=mx\n",
    "    # img = np.transpose(img,(2,0,1))\n",
    "    img_ten = torch.tensor(img)\n",
    "    return img_ten\n",
    "\n",
    "def preprocess_mask(path):\n",
    "    \n",
    "    msk = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    msk = msk.astype('float32')\n",
    "    msk/=255.0\n",
    "    msk_ten = torch.tensor(msk)\n",
    "    \n",
    "    return msk_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenNetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_files, mask_files, input_size=(16, 256, 256), augmentation_transforms=None):\n",
    "        self.image_files=image_files\n",
    "        self.mask_files=mask_files\n",
    "        self.input_D = input_size[0]\n",
    "        self.input_H = input_size[1]\n",
    "        self.input_W = input_size[2]\n",
    "        self.augmentation_transforms=augmentation_transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get 16 images instead of 1\n",
    "        N = len(self.image_files)\n",
    "        if idx + self.input_D > N:\n",
    "            diff = idx + self.input_D - N\n",
    "            start_idx = idx - diff\n",
    "            image_paths = self.image_files[start_idx:N]\n",
    "            mask_paths = self.mask_files[start_idx:N]\n",
    "        elif idx <= self.input_D:\n",
    "            diff = self.input_D - idx\n",
    "            end_range = self.input_D + diff\n",
    "            image_paths = self.image_files[idx:end_range]\n",
    "            mask_paths = self.mask_files[idx:end_range]\n",
    "\n",
    "        # Extract images into tensor\n",
    "        images = torch.stack(list(map(preprocess_image, image_paths)), dim=0)\n",
    "        masks = torch.stack(list(map(preprocess_mask, mask_paths)), dim=0)\n",
    "        img_size = list(images.size())\n",
    "        mask_size = list(masks.size())\n",
    "        print(img_size)\n",
    "\n",
    "        images = images.reshape((1, img_size[0], img_size[1], img_size[2]))\n",
    "        masks = masks.reshape((1, mask_size[0], mask_size[1], mask_size[2]))\n",
    "\n",
    "        # image, mask = self.__training_data_process__(img, mask)\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            image, mask=self.augmentation_transforms(image, mask, self.input_size)\n",
    "\n",
    "        # make sure it is tensor of shape: [1, z, y, x]\n",
    "        assert images.shape == masks.shape, \"img shape:{} is not equal to mask shape:{}\".format(images.shape, masks.shape)\n",
    "        return images, masks\n",
    "\n",
    "    def __drop_invalid_range__(self, volume, label=None):\n",
    "        \"\"\"\n",
    "        Cut off the invalid area\n",
    "        \"\"\"\n",
    "        zero_value = volume[0, 0, 0]\n",
    "        non_zeros_idx = np.where(volume != zero_value)\n",
    "        \n",
    "        [max_z, max_h, max_w] = np.max(np.array(non_zeros_idx), axis=1)\n",
    "        [min_z, min_h, min_w] = np.min(np.array(non_zeros_idx), axis=1)\n",
    "        \n",
    "        if label is not None:\n",
    "            return volume[min_z:max_z, min_h:max_h, min_w:max_w], label[min_z:max_z, min_h:max_h, min_w:max_w]\n",
    "        else:\n",
    "            return volume[min_z:max_z, min_h:max_h, min_w:max_w]\n",
    "\n",
    "\n",
    "    def __random_center_crop__(self, data, label):\n",
    "        from random import random\n",
    "        \"\"\"\n",
    "        Random crop\n",
    "        \"\"\"\n",
    "        target_indexs = np.where(label>0)\n",
    "        [img_d, img_h, img_w] = data.shape\n",
    "        [max_D, max_H, max_W] = np.max(np.array(target_indexs), axis=1)\n",
    "        [min_D, min_H, min_W] = np.min(np.array(target_indexs), axis=1)\n",
    "        [target_depth, target_height, target_width] = np.array([max_D, max_H, max_W]) - np.array([min_D, min_H, min_W])\n",
    "        Z_min = int((min_D - target_depth*1.0/2) * random())\n",
    "        Y_min = int((min_H - target_height*1.0/2) * random())\n",
    "        X_min = int((min_W - target_width*1.0/2) * random())\n",
    "        \n",
    "        Z_max = int(img_d - ((img_d - (max_D + target_depth*1.0/2)) * random()))\n",
    "        Y_max = int(img_h - ((img_h - (max_H + target_height*1.0/2)) * random()))\n",
    "        X_max = int(img_w - ((img_w - (max_W + target_width*1.0/2)) * random()))\n",
    "       \n",
    "        Z_min = np.max([0, Z_min])\n",
    "        Y_min = np.max([0, Y_min])\n",
    "        X_min = np.max([0, X_min])\n",
    "\n",
    "        Z_max = np.min([img_d, Z_max])\n",
    "        Y_max = np.min([img_h, Y_max])\n",
    "        X_max = np.min([img_w, X_max])\n",
    " \n",
    "        Z_min = int(Z_min)\n",
    "        Y_min = int(Y_min)\n",
    "        X_min = int(X_min)\n",
    "        \n",
    "        Z_max = int(Z_max)\n",
    "        Y_max = int(Y_max)\n",
    "        X_max = int(X_max)\n",
    "\n",
    "        return data[Z_min: Z_max, Y_min: Y_max, X_min: X_max], label[Z_min: Z_max, Y_min: Y_max, X_min: X_max]\n",
    "\n",
    "\n",
    "\n",
    "    def __itensity_normalize_one_volume__(self, volume):\n",
    "        \"\"\"\n",
    "        normalize the itensity of an nd volume based on the mean and std of nonzeor region\n",
    "        inputs:\n",
    "            volume: the input nd volume\n",
    "        outputs:\n",
    "            out: the normalized nd volume\n",
    "        \"\"\"\n",
    "        \n",
    "        pixels = volume[volume > 0]\n",
    "        mean = pixels.mean()\n",
    "        std  = pixels.std()\n",
    "        out = (volume - mean)/std\n",
    "        out_random = np.random.normal(0, 1, size = volume.shape)\n",
    "        out[volume == 0] = out_random[volume == 0]\n",
    "        return out\n",
    "\n",
    "    def __resize_data__(self, data):\n",
    "        \"\"\"\n",
    "        Resize the data to the input size\n",
    "        \"\"\" \n",
    "        [depth, height, width] = data.shape\n",
    "        scale = [self.input_D*1.0/depth, self.input_H*1.0/height, self.input_W*1.0/width]  \n",
    "        # data = ndimage.interpolation.zoom(data, scale, order=0)\n",
    "        raise NotImplementedError(\"Please add scipy as a dep\")\n",
    "        #return data\n",
    "\n",
    "\n",
    "    def __crop_data__(self, data, label):\n",
    "        \"\"\"\n",
    "        Random crop with different methods:\n",
    "        \"\"\" \n",
    "        # random center crop\n",
    "        data, label = self.__random_center_crop__ (data, label)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __training_data_process__(self, data, label): \n",
    "        # crop data according net input size\n",
    "        data = data.get_data()\n",
    "        label = label.get_data()\n",
    "        \n",
    "        # drop out the invalid range\n",
    "        data, label = self.__drop_invalid_range__(data, label)\n",
    "        \n",
    "        # crop data\n",
    "        data, label = self.__crop_data__(data, label) \n",
    "\n",
    "        # resize data\n",
    "        data = self.__resize_data__(data)\n",
    "        label = self.__resize_data__(label)\n",
    "\n",
    "        # normalization datas\n",
    "        data = self.__itensity_normalize_one_volume__(data)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files, val_image_files, train_mask_files, val_mask_files = train_test_split(\n",
    "    image_files, label_files, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = SenNetDataset(train_image_files, train_mask_files, input_size=(8, 256, 256))\n",
    "val_dataset = SenNetDataset(val_image_files, val_mask_files, input_size=(8, 256, 256))\n",
    "\n",
    "train_dataloader= DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/segment_vasculature/models/tencent_model/resnet.py:173: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.conv1.weight\n",
      "module.bn1.weight\n",
      "module.bn1.bias\n",
      "module.bn1.running_mean\n",
      "module.bn1.running_var\n",
      "module.bn1.num_batches_tracked\n",
      "module.layer1.0.conv1.weight\n",
      "module.layer1.0.bn1.weight\n",
      "module.layer1.0.bn1.bias\n",
      "module.layer1.0.bn1.running_mean\n",
      "module.layer1.0.bn1.running_var\n",
      "module.layer1.0.bn1.num_batches_tracked\n",
      "module.layer1.0.conv2.weight\n",
      "module.layer1.0.bn2.weight\n",
      "module.layer1.0.bn2.bias\n",
      "module.layer1.0.bn2.running_mean\n",
      "module.layer1.0.bn2.running_var\n",
      "module.layer1.0.bn2.num_batches_tracked\n",
      "module.layer1.0.conv3.weight\n",
      "module.layer1.0.bn3.weight\n",
      "module.layer1.0.bn3.bias\n",
      "module.layer1.0.bn3.running_mean\n",
      "module.layer1.0.bn3.running_var\n",
      "module.layer1.0.bn3.num_batches_tracked\n",
      "module.layer1.0.downsample.0.weight\n",
      "module.layer1.0.downsample.1.weight\n",
      "module.layer1.0.downsample.1.bias\n",
      "module.layer1.0.downsample.1.running_mean\n",
      "module.layer1.0.downsample.1.running_var\n",
      "module.layer1.0.downsample.1.num_batches_tracked\n",
      "module.layer1.1.conv1.weight\n",
      "module.layer1.1.bn1.weight\n",
      "module.layer1.1.bn1.bias\n",
      "module.layer1.1.bn1.running_mean\n",
      "module.layer1.1.bn1.running_var\n",
      "module.layer1.1.bn1.num_batches_tracked\n",
      "module.layer1.1.conv2.weight\n",
      "module.layer1.1.bn2.weight\n",
      "module.layer1.1.bn2.bias\n",
      "module.layer1.1.bn2.running_mean\n",
      "module.layer1.1.bn2.running_var\n",
      "module.layer1.1.bn2.num_batches_tracked\n",
      "module.layer1.1.conv3.weight\n",
      "module.layer1.1.bn3.weight\n",
      "module.layer1.1.bn3.bias\n",
      "module.layer1.1.bn3.running_mean\n",
      "module.layer1.1.bn3.running_var\n",
      "module.layer1.1.bn3.num_batches_tracked\n",
      "module.layer1.2.conv1.weight\n",
      "module.layer1.2.bn1.weight\n",
      "module.layer1.2.bn1.bias\n",
      "module.layer1.2.bn1.running_mean\n",
      "module.layer1.2.bn1.running_var\n",
      "module.layer1.2.bn1.num_batches_tracked\n",
      "module.layer1.2.conv2.weight\n",
      "module.layer1.2.bn2.weight\n",
      "module.layer1.2.bn2.bias\n",
      "module.layer1.2.bn2.running_mean\n",
      "module.layer1.2.bn2.running_var\n",
      "module.layer1.2.bn2.num_batches_tracked\n",
      "module.layer1.2.conv3.weight\n",
      "module.layer1.2.bn3.weight\n",
      "module.layer1.2.bn3.bias\n",
      "module.layer1.2.bn3.running_mean\n",
      "module.layer1.2.bn3.running_var\n",
      "module.layer1.2.bn3.num_batches_tracked\n",
      "module.layer2.0.conv1.weight\n",
      "module.layer2.0.bn1.weight\n",
      "module.layer2.0.bn1.bias\n",
      "module.layer2.0.bn1.running_mean\n",
      "module.layer2.0.bn1.running_var\n",
      "module.layer2.0.bn1.num_batches_tracked\n",
      "module.layer2.0.conv2.weight\n",
      "module.layer2.0.bn2.weight\n",
      "module.layer2.0.bn2.bias\n",
      "module.layer2.0.bn2.running_mean\n",
      "module.layer2.0.bn2.running_var\n",
      "module.layer2.0.bn2.num_batches_tracked\n",
      "module.layer2.0.conv3.weight\n",
      "module.layer2.0.bn3.weight\n",
      "module.layer2.0.bn3.bias\n",
      "module.layer2.0.bn3.running_mean\n",
      "module.layer2.0.bn3.running_var\n",
      "module.layer2.0.bn3.num_batches_tracked\n",
      "module.layer2.0.downsample.0.weight\n",
      "module.layer2.0.downsample.1.weight\n",
      "module.layer2.0.downsample.1.bias\n",
      "module.layer2.0.downsample.1.running_mean\n",
      "module.layer2.0.downsample.1.running_var\n",
      "module.layer2.0.downsample.1.num_batches_tracked\n",
      "module.layer2.1.conv1.weight\n",
      "module.layer2.1.bn1.weight\n",
      "module.layer2.1.bn1.bias\n",
      "module.layer2.1.bn1.running_mean\n",
      "module.layer2.1.bn1.running_var\n",
      "module.layer2.1.bn1.num_batches_tracked\n",
      "module.layer2.1.conv2.weight\n",
      "module.layer2.1.bn2.weight\n",
      "module.layer2.1.bn2.bias\n",
      "module.layer2.1.bn2.running_mean\n",
      "module.layer2.1.bn2.running_var\n",
      "module.layer2.1.bn2.num_batches_tracked\n",
      "module.layer2.1.conv3.weight\n",
      "module.layer2.1.bn3.weight\n",
      "module.layer2.1.bn3.bias\n",
      "module.layer2.1.bn3.running_mean\n",
      "module.layer2.1.bn3.running_var\n",
      "module.layer2.1.bn3.num_batches_tracked\n",
      "module.layer2.2.conv1.weight\n",
      "module.layer2.2.bn1.weight\n",
      "module.layer2.2.bn1.bias\n",
      "module.layer2.2.bn1.running_mean\n",
      "module.layer2.2.bn1.running_var\n",
      "module.layer2.2.bn1.num_batches_tracked\n",
      "module.layer2.2.conv2.weight\n",
      "module.layer2.2.bn2.weight\n",
      "module.layer2.2.bn2.bias\n",
      "module.layer2.2.bn2.running_mean\n",
      "module.layer2.2.bn2.running_var\n",
      "module.layer2.2.bn2.num_batches_tracked\n",
      "module.layer2.2.conv3.weight\n",
      "module.layer2.2.bn3.weight\n",
      "module.layer2.2.bn3.bias\n",
      "module.layer2.2.bn3.running_mean\n",
      "module.layer2.2.bn3.running_var\n",
      "module.layer2.2.bn3.num_batches_tracked\n",
      "module.layer2.3.conv1.weight\n",
      "module.layer2.3.bn1.weight\n",
      "module.layer2.3.bn1.bias\n",
      "module.layer2.3.bn1.running_mean\n",
      "module.layer2.3.bn1.running_var\n",
      "module.layer2.3.bn1.num_batches_tracked\n",
      "module.layer2.3.conv2.weight\n",
      "module.layer2.3.bn2.weight\n",
      "module.layer2.3.bn2.bias\n",
      "module.layer2.3.bn2.running_mean\n",
      "module.layer2.3.bn2.running_var\n",
      "module.layer2.3.bn2.num_batches_tracked\n",
      "module.layer2.3.conv3.weight\n",
      "module.layer2.3.bn3.weight\n",
      "module.layer2.3.bn3.bias\n",
      "module.layer2.3.bn3.running_mean\n",
      "module.layer2.3.bn3.running_var\n",
      "module.layer2.3.bn3.num_batches_tracked\n",
      "module.layer3.0.conv1.weight\n",
      "module.layer3.0.bn1.weight\n",
      "module.layer3.0.bn1.bias\n",
      "module.layer3.0.bn1.running_mean\n",
      "module.layer3.0.bn1.running_var\n",
      "module.layer3.0.bn1.num_batches_tracked\n",
      "module.layer3.0.conv2.weight\n",
      "module.layer3.0.bn2.weight\n",
      "module.layer3.0.bn2.bias\n",
      "module.layer3.0.bn2.running_mean\n",
      "module.layer3.0.bn2.running_var\n",
      "module.layer3.0.bn2.num_batches_tracked\n",
      "module.layer3.0.conv3.weight\n",
      "module.layer3.0.bn3.weight\n",
      "module.layer3.0.bn3.bias\n",
      "module.layer3.0.bn3.running_mean\n",
      "module.layer3.0.bn3.running_var\n",
      "module.layer3.0.bn3.num_batches_tracked\n",
      "module.layer3.0.downsample.0.weight\n",
      "module.layer3.0.downsample.1.weight\n",
      "module.layer3.0.downsample.1.bias\n",
      "module.layer3.0.downsample.1.running_mean\n",
      "module.layer3.0.downsample.1.running_var\n",
      "module.layer3.0.downsample.1.num_batches_tracked\n",
      "module.layer3.1.conv1.weight\n",
      "module.layer3.1.bn1.weight\n",
      "module.layer3.1.bn1.bias\n",
      "module.layer3.1.bn1.running_mean\n",
      "module.layer3.1.bn1.running_var\n",
      "module.layer3.1.bn1.num_batches_tracked\n",
      "module.layer3.1.conv2.weight\n",
      "module.layer3.1.bn2.weight\n",
      "module.layer3.1.bn2.bias\n",
      "module.layer3.1.bn2.running_mean\n",
      "module.layer3.1.bn2.running_var\n",
      "module.layer3.1.bn2.num_batches_tracked\n",
      "module.layer3.1.conv3.weight\n",
      "module.layer3.1.bn3.weight\n",
      "module.layer3.1.bn3.bias\n",
      "module.layer3.1.bn3.running_mean\n",
      "module.layer3.1.bn3.running_var\n",
      "module.layer3.1.bn3.num_batches_tracked\n",
      "module.layer3.2.conv1.weight\n",
      "module.layer3.2.bn1.weight\n",
      "module.layer3.2.bn1.bias\n",
      "module.layer3.2.bn1.running_mean\n",
      "module.layer3.2.bn1.running_var\n",
      "module.layer3.2.bn1.num_batches_tracked\n",
      "module.layer3.2.conv2.weight\n",
      "module.layer3.2.bn2.weight\n",
      "module.layer3.2.bn2.bias\n",
      "module.layer3.2.bn2.running_mean\n",
      "module.layer3.2.bn2.running_var\n",
      "module.layer3.2.bn2.num_batches_tracked\n",
      "module.layer3.2.conv3.weight\n",
      "module.layer3.2.bn3.weight\n",
      "module.layer3.2.bn3.bias\n",
      "module.layer3.2.bn3.running_mean\n",
      "module.layer3.2.bn3.running_var\n",
      "module.layer3.2.bn3.num_batches_tracked\n",
      "module.layer3.3.conv1.weight\n",
      "module.layer3.3.bn1.weight\n",
      "module.layer3.3.bn1.bias\n",
      "module.layer3.3.bn1.running_mean\n",
      "module.layer3.3.bn1.running_var\n",
      "module.layer3.3.bn1.num_batches_tracked\n",
      "module.layer3.3.conv2.weight\n",
      "module.layer3.3.bn2.weight\n",
      "module.layer3.3.bn2.bias\n",
      "module.layer3.3.bn2.running_mean\n",
      "module.layer3.3.bn2.running_var\n",
      "module.layer3.3.bn2.num_batches_tracked\n",
      "module.layer3.3.conv3.weight\n",
      "module.layer3.3.bn3.weight\n",
      "module.layer3.3.bn3.bias\n",
      "module.layer3.3.bn3.running_mean\n",
      "module.layer3.3.bn3.running_var\n",
      "module.layer3.3.bn3.num_batches_tracked\n",
      "module.layer3.4.conv1.weight\n",
      "module.layer3.4.bn1.weight\n",
      "module.layer3.4.bn1.bias\n",
      "module.layer3.4.bn1.running_mean\n",
      "module.layer3.4.bn1.running_var\n",
      "module.layer3.4.bn1.num_batches_tracked\n",
      "module.layer3.4.conv2.weight\n",
      "module.layer3.4.bn2.weight\n",
      "module.layer3.4.bn2.bias\n",
      "module.layer3.4.bn2.running_mean\n",
      "module.layer3.4.bn2.running_var\n",
      "module.layer3.4.bn2.num_batches_tracked\n",
      "module.layer3.4.conv3.weight\n",
      "module.layer3.4.bn3.weight\n",
      "module.layer3.4.bn3.bias\n",
      "module.layer3.4.bn3.running_mean\n",
      "module.layer3.4.bn3.running_var\n",
      "module.layer3.4.bn3.num_batches_tracked\n",
      "module.layer3.5.conv1.weight\n",
      "module.layer3.5.bn1.weight\n",
      "module.layer3.5.bn1.bias\n",
      "module.layer3.5.bn1.running_mean\n",
      "module.layer3.5.bn1.running_var\n",
      "module.layer3.5.bn1.num_batches_tracked\n",
      "module.layer3.5.conv2.weight\n",
      "module.layer3.5.bn2.weight\n",
      "module.layer3.5.bn2.bias\n",
      "module.layer3.5.bn2.running_mean\n",
      "module.layer3.5.bn2.running_var\n",
      "module.layer3.5.bn2.num_batches_tracked\n",
      "module.layer3.5.conv3.weight\n",
      "module.layer3.5.bn3.weight\n",
      "module.layer3.5.bn3.bias\n",
      "module.layer3.5.bn3.running_mean\n",
      "module.layer3.5.bn3.running_var\n",
      "module.layer3.5.bn3.num_batches_tracked\n",
      "module.layer4.0.conv1.weight\n",
      "module.layer4.0.bn1.weight\n",
      "module.layer4.0.bn1.bias\n",
      "module.layer4.0.bn1.running_mean\n",
      "module.layer4.0.bn1.running_var\n",
      "module.layer4.0.bn1.num_batches_tracked\n",
      "module.layer4.0.conv2.weight\n",
      "module.layer4.0.bn2.weight\n",
      "module.layer4.0.bn2.bias\n",
      "module.layer4.0.bn2.running_mean\n",
      "module.layer4.0.bn2.running_var\n",
      "module.layer4.0.bn2.num_batches_tracked\n",
      "module.layer4.0.conv3.weight\n",
      "module.layer4.0.bn3.weight\n",
      "module.layer4.0.bn3.bias\n",
      "module.layer4.0.bn3.running_mean\n",
      "module.layer4.0.bn3.running_var\n",
      "module.layer4.0.bn3.num_batches_tracked\n",
      "module.layer4.0.downsample.0.weight\n",
      "module.layer4.0.downsample.1.weight\n",
      "module.layer4.0.downsample.1.bias\n",
      "module.layer4.0.downsample.1.running_mean\n",
      "module.layer4.0.downsample.1.running_var\n",
      "module.layer4.0.downsample.1.num_batches_tracked\n",
      "module.layer4.1.conv1.weight\n",
      "module.layer4.1.bn1.weight\n",
      "module.layer4.1.bn1.bias\n",
      "module.layer4.1.bn1.running_mean\n",
      "module.layer4.1.bn1.running_var\n",
      "module.layer4.1.bn1.num_batches_tracked\n",
      "module.layer4.1.conv2.weight\n",
      "module.layer4.1.bn2.weight\n",
      "module.layer4.1.bn2.bias\n",
      "module.layer4.1.bn2.running_mean\n",
      "module.layer4.1.bn2.running_var\n",
      "module.layer4.1.bn2.num_batches_tracked\n",
      "module.layer4.1.conv3.weight\n",
      "module.layer4.1.bn3.weight\n",
      "module.layer4.1.bn3.bias\n",
      "module.layer4.1.bn3.running_mean\n",
      "module.layer4.1.bn3.running_var\n",
      "module.layer4.1.bn3.num_batches_tracked\n",
      "module.layer4.2.conv1.weight\n",
      "module.layer4.2.bn1.weight\n",
      "module.layer4.2.bn1.bias\n",
      "module.layer4.2.bn1.running_mean\n",
      "module.layer4.2.bn1.running_var\n",
      "module.layer4.2.bn1.num_batches_tracked\n",
      "module.layer4.2.conv2.weight\n",
      "module.layer4.2.bn2.weight\n",
      "module.layer4.2.bn2.bias\n",
      "module.layer4.2.bn2.running_mean\n",
      "module.layer4.2.bn2.running_var\n",
      "module.layer4.2.bn2.num_batches_tracked\n",
      "module.layer4.2.conv3.weight\n",
      "module.layer4.2.bn3.weight\n",
      "module.layer4.2.bn3.bias\n",
      "module.layer4.2.bn3.running_mean\n",
      "module.layer4.2.bn3.running_var\n",
      "module.layer4.2.bn3.num_batches_tracked\n",
      "module.conv_seg.0.weight\n",
      "module.conv_seg.0.bias\n",
      "module.conv_seg.1.weight\n",
      "module.conv_seg.1.bias\n",
      "module.conv_seg.1.running_mean\n",
      "module.conv_seg.1.running_var\n",
      "module.conv_seg.1.num_batches_tracked\n",
      "module.conv_seg.3.weight\n",
      "module.conv_seg.4.weight\n",
      "module.conv_seg.4.bias\n",
      "module.conv_seg.4.running_mean\n",
      "module.conv_seg.4.running_var\n",
      "module.conv_seg.4.num_batches_tracked\n",
      "module.conv_seg.6.weight\n"
     ]
    }
   ],
   "source": [
    "kw = {'sample_input_D': 8, 'sample_input_H': 256, 'sample_input_W': 256, 'num_seg_classes': 1}\n",
    "model = resnet50(**kw)\n",
    "st_dict = torch.load('tencent_model/trails/models/resnet_50_epoch_110_batch_0.pth.tar')\n",
    "\n",
    "new_state_dict = dict()\n",
    "for k, v in st_dict[\"state_dict\"].items():\n",
    "        # Strip module. from start of keys\n",
    "        new_key = k.strip(\"module\").strip(\".\")\n",
    "        if \"num_batches_track\" in new_key:\n",
    "            continue\n",
    "\n",
    "        new_state_dict[new_key] = v\n",
    "for k in st_dict[\"state_dict\"].keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "[16, 1041, 1511]\n",
      "torch.Size([1, 1, 16, 1041, 1511])\n",
      "torch.Size([1, 2, 4, 262, 378])\n",
      "torch.Size([1, 1, 16, 1041, 1511])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (792288) must match the size of tensor b (25167216) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     trained_model, train_epoch_losses, test_epoch_losses \u001b[38;5;241m=\u001b[39m train_and_test(model, dataloaders, optimizer, criterion, num_epochs\u001b[38;5;241m=\u001b[39mepochs, show_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_model, train_epoch_losses, test_epoch_losses\n\u001b[0;32m---> 23\u001b[0m trained_model, train_epoch_losses, test_epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#torch.save(trained_model.state_dict(), 'lower_learning_rate_100.pth')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     18\u001b[0m criterion \u001b[38;5;241m=\u001b[39m DiceLoss()\n\u001b[0;32m---> 19\u001b[0m trained_model, train_epoch_losses, test_epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_model, train_epoch_losses, test_epoch_losses\n",
      "File \u001b[0;32m/workspaces/segment_vasculature/models/helpers/train_test.py:52\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, dataloaders, optimizer, criterion, num_epochs, show_images)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m     55\u001b[0m y_true \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/segment_vasculature/models/helpers/loss_functions.py:16\u001b[0m, in \u001b[0;36mDiceLoss.forward\u001b[0;34m(self, inputs, targets, smooth)\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m intersection \u001b[38;5;241m=\u001b[39m (\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m)\u001b[38;5;241m.\u001b[39msum()                            \n\u001b[1;32m     17\u001b[0m dice \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.\u001b[39m\u001b[38;5;241m*\u001b[39mintersection \u001b[38;5;241m+\u001b[39m smooth)\u001b[38;5;241m/\u001b[39m(inputs\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m targets\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m smooth)  \n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dice\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (792288) must match the size of tensor b (25167216) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "def train():\n",
    "    kw = {'sample_input_D': 1, 'sample_input_H': 256, 'sample_input_W': 256, 'num_seg_classes': 2}\n",
    "    model = resnet50(**kw)\n",
    "    st_dict = torch.load('tencent_model/trails/models/resnet_50_epoch_110_batch_0.pth.tar')\n",
    "\n",
    "    new_state_dict = dict()\n",
    "    for k, v in st_dict[\"state_dict\"].items():\n",
    "            # Strip module. from start of keys\n",
    "            new_key = k.strip(\"module\").strip(\".\")\n",
    "            if \"num_batches_track\" in new_key:\n",
    "                continue\n",
    "\n",
    "            new_state_dict[new_key] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = DiceLoss()\n",
    "    trained_model, train_epoch_losses, test_epoch_losses = train_and_test(model, dataloaders, optimizer, criterion, num_epochs=epochs, show_images=True)\n",
    "    return trained_model, train_epoch_losses, test_epoch_losses\n",
    "\n",
    "\n",
    "trained_model, train_epoch_losses, test_epoch_losses = train()\n",
    "#torch.save(trained_model.state_dict(), 'lower_learning_rate_100.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
